import { Injectable, Logger } from '@nestjs/common';
import { PrismaClient } from '@prisma/client';
import * as OpenAI from 'openai';

const prisma = new PrismaClient();

@Injectable()
export class TranscriptionService {
  private readonly openai: OpenAI;
  private readonly logger = new Logger(TranscriptionService.name);

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  async startTranscription(sessionId: string, recordingId?: string) {
    const session = await prisma.liveSession.findUnique({
      where: { id: sessionId },
    });

    if (!session) {
      throw new Error('Session not found');
    }

    // Create transcription record
    const transcription = await prisma.transcription.create({
      data: {
        sessionId,
        recordingId,
        status: 'PROCESSING',
      },
    });

    // Start async transcription process
    this.processTranscription(transcription.id, session.recordingUrl);

    return transcription;
  }

  async processTranscription(transcriptionId: string, audioUrl: string) {
    try {
      this.logger.log(`Starting transcription for ${transcriptionId}`);

      // Update progress
      await prisma.transcription.update({
        where: { id: transcriptionId },
        data: { progress: 10 },
      });

      // Download audio file (simplified - in production, use proper download)
      // const audioBuffer = await this.downloadAudio(audioUrl);

      // For demo, we'll use a mock
      const mockTranscript = {
        text: 'This is a mock transcription. In production, this would be generated by OpenAI Whisper.',
        segments: [
          {
            start: 0,
            end: 2.5,
            text: 'Welcome everyone to our yoga session.',
            speaker: 'instructor',
          },
          {
            start: 2.5,
            end: 5.0,
            text: 'Today we will focus on morning flow.',
            speaker: 'instructor',
          },
        ],
      };

      // In production, use:
      // const transcription = await this.openai.audio.transcriptions.create({
      //   file: audioBuffer,
      //   model: 'whisper-1',
      //   response_format: 'verbose_json',
      //   timestamp_granularities: ['segment', 'word'],
      // });

      // Simulate processing time
      await new Promise(resolve => setTimeout(resolve, 3000));

      // Update with results
      await prisma.transcription.update({
        where: { id: transcriptionId },
        data: {
          text: mockTranscript.text,
          segments: mockTranscript.segments,
          status: 'COMPLETED',
          progress: 100,
          processedAt: new Date(),
        },
      });

      this.logger.log(`Transcription completed for ${transcriptionId}`);
    } catch (error) {
      this.logger.error(`Transcription failed for ${transcriptionId}:`, error);
      
      await prisma.transcription.update({
        where: { id: transcriptionId },
        data: {
          status: 'FAILED',
        },
      });
    }
  }

  async getTranscription(transcriptionId: string) {
    const transcription = await prisma.transcription.findUnique({
      where: { id: transcriptionId },
    });

    if (!transcription) {
      throw new Error('Transcription not found');
    }

    return transcription;
  }

  async searchInTranscription(sessionId: string, query: string) {
    const transcriptions = await prisma.transcription.findMany({
      where: {
        sessionId,
        status: 'COMPLETED',
        text: {
          contains: query,
          mode: 'insensitive',
        },
      },
    });

    // For each transcription, find matching segments
    const results = await Promise.all(
      transcriptions.map(async (transcription) => {
        const segments = transcription.segments as any[];
        const matchingSegments = segments.filter(segment =>
          segment.text.toLowerCase().includes(query.toLowerCase())
        );

        return {
          transcriptionId: transcription.id,
          matches: matchingSegments.map(segment => ({
            text: segment.text,
            start: segment.start,
            end: segment.end,
            speaker: segment.speaker,
          })),
        };
      })
    );

    return results.filter(result => result.matches.length > 0);
  }

  async generateSummary(transcriptionId: string) {
    const transcription = await this.getTranscription(transcriptionId);

    if (!transcription.text) {
      throw new Error('Transcription text not available');
    }

    // In production, use GPT to generate summary
    // const summary = await this.openai.chat.completions.create({
    //   model: 'gpt-4',
    //   messages: [
    //     {
    //       role: 'system',
    //       content: 'Summarize this yoga session transcription into key points.',
    //     },
    //     {
    //       role: 'user',
    //       content: transcription.text,
    //     },
    //   ],
    // });

    // Mock summary for demo
    const mockSummary = {
      keyPoints: [
        'Morning yoga flow session focused on energizing poses',
        'Emphasis on breath synchronization',
        'Sun salutation sequence practiced',
        'Cool down with meditation',
      ],
      duration: '60 minutes',
      difficulty: 'Intermediate',
      instructorTips: ['Focus on breathing', 'Listen to your body'],
    };

    return mockSummary;
  }

  async exportTranscription(transcriptionId: string, format: 'txt' | 'srt' | 'vtt') {
    const transcription = await this.getTranscription(transcriptionId);

    let exportContent = '';

    switch (format) {
      case 'txt':
        exportContent = transcription.text || '';
        break;
      case 'srt':
        exportContent = this.convertToSRT(transcription.segments as any[]);
        break;
      case 'vtt':
        exportContent = this.convertToVTT(transcription.segments as any[]);
        break;
    }

    return {
      content: exportContent,
      format,
      fileName: `transcription-${transcriptionId}.${format}`,
    };
  }

  private convertToSRT(segments: any[]): string {
    let srt = '';
    segments.forEach((segment, index) => {
      srt += `${index + 1}\n`;
      srt += `${this.formatTime(segment.start)} --> ${this.formatTime(segment.end)}\n`;
      srt += `${segment.text}\n\n`;
    });
    return srt;
  }

  private convertToVTT(segments: any[]): string {
    let vtt = 'WEBVTT\n\n';
    segments.forEach((segment, index) => {
      vtt += `${this.formatTimeVTT(segment.start)} --> ${this.formatTimeVTT(segment.end)}\n`;
      vtt += `${segment.text}\n\n`;
    });
    return vtt;
  }

  private formatTime(seconds: number): string {
    const date = new Date(0);
    date.setSeconds(seconds);
    return date.toISOString().substr(11, 12).replace('.', ',');
  }

  private formatTimeVTT(seconds: number): string {
    const date = new Date(0);
    date.setSeconds(seconds);
    return date.toISOString().substr(11, 12);
  }
}